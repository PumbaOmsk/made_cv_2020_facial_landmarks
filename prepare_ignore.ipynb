{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка данных\n",
    "Решил посмотреть как выглядят картинки на которых мы ошибаемся больше всего и увидел, что `train` содержит изображения со сдвинутой разметкой. \n",
    "\n",
    "Для их обнаружения обучается такая же модель, но на всех картинках, включая *испорченные*. После этого перебираются все изображения и делается на них предсказание. Если MSE больше 100 (просто константа), считаем, что на этой картинке неадекватная разметка.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "from utils import NUM_PTS, CROP_SIZE\n",
    "from utils import train,validate,predict\n",
    "from utils import ScaleMinSideToSize,CropCenter,TransformByKeys\n",
    "from utils import ThousandLandmarksDataset\n",
    "from utils import restore_landmarks_batch, create_submission,restore_landmarks_batch_ex,draw_landmarks\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "use_gpu = True\n",
    "data_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/_Data/full/\"\n",
    "learning_rate = 1e-3\n",
    "batch_size = 192\n",
    "epochs = 30\n",
    "prj_name = \"test\"\n",
    "# data_size = 40000\n",
    "# data_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метрика, которая учитывает масштабирование изображений\n",
    "class MseW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MseW,self).__init__()\n",
    "\n",
    "    def setWeight(self, weight):\n",
    "        self.w = weight\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        mse = torch.mul(outputs - labels,outputs - labels).mean(axis=1)        \n",
    "        mse=torch.mul(mse,self.w).mean(axis=0)\n",
    "        mse=mse.mean(axis=0)\n",
    "        mse=2*mse\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropRandom(object):\n",
    "    def __init__(self, size=CROP_SIZE, elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if 'landmarks' in sample:\n",
    "            img = sample[self.elem_name] #.copy()\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            bound=landmarks[:,0].min(),landmarks[:,1].min(),landmarks[:,0].max(),landmarks[:,1].max()                        \n",
    "            h,w,_ = img.shape\n",
    "            min_sq = max(bound[3]-bound[1],bound[2]-bound[0])\n",
    "            max_sq = min(w,h)            \n",
    "            if min_sq+1<max_sq-1:\n",
    "                sq = np.random.randint(min_sq+1,max_sq-1)\n",
    "            else:\n",
    "                sq = max_sq-1                \n",
    "            \n",
    "            min_dx = max(bound[2]-sq,0)\n",
    "            max_dx = min(w-sq,bound[0])\n",
    "            if min_dx<max_dx:\n",
    "                dx = np.random.randint(min_dx,max_dx)\n",
    "            else:\n",
    "                dx = int(min_dx)\n",
    "            \n",
    "            min_dy = max(bound[3]-sq,0)\n",
    "            max_dy = min(h-sq,bound[1])\n",
    "            if min_dy<max_dy:\n",
    "                dy = np.random.randint(min_dy,max_dy)\n",
    "            else:                \n",
    "                dy = int(min_dy)\n",
    "                \n",
    "            landmarks -= torch.tensor((dx, dy), dtype=landmarks.dtype)[None, :]                        \n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "            sample[self.elem_name] = img[dy:dy+sq, dx:dx+sq]\n",
    "            sample['dx'] = torch.tensor(dx,dtype=torch.short)\n",
    "            sample['dy'] = torch.tensor(dy,dtype=torch.short)\n",
    "        else:\n",
    "            raise RuntimeError(f\"stop\")\n",
    "            sample['dx'] = torch.tensor(0)\n",
    "            sample['dy'] = torch.tensor(0)\n",
    "        return sample    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fn = fnn.mse_loss\n",
    "valid_loss_fn = MseW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline при обучении\n",
    "train_transforms = transforms.Compose([\n",
    "#     RandomFlipV(),\n",
    "    CropRandom(),    \n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),   \n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])\n",
    "# pipeline при валидации и предсказании\n",
    "val_transforms = transforms.Compose([\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Reading data...\")\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), train_transforms, split=\"train\",size = data_size) \n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=True,\n",
    "                                   shuffle=True)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), val_transforms, split=\"val\",size = data_size)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=False,shuffle=False)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\")\n",
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. train & validate\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(0,epochs):    \n",
    "    train_loss = train(model, train_dataloader, train_loss_fn, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, valid_loss_fn, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.4f}\\tval loss: {:5.4f}\".format(epoch, train_loss, val_loss))    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"{prj_name}_best.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "    with open(f\"{prj_name}_\"+str(epoch)+\".pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предскажем значения для валидационного множества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_landmark_to_pred(ds):\n",
    "    lm = ds['landmarks'].numpy().copy()\n",
    "    dx = ds['crop_margin_x']\n",
    "    dy = ds['crop_margin_y']    \n",
    "    for ix in range(0,len(lm),2):\n",
    "        lm[ix]+=dx\n",
    "    for iy in range(1,len(lm),2):\n",
    "        lm[iy]+=dy\n",
    "    coef = ds['scale_coef'].numpy()    \n",
    "    lm=lm/coef\n",
    "    lm = lm.reshape(-1,2)\n",
    "    return lm\n",
    "\n",
    "def calc_err(idx,landmarks,val_dataset,loss_fn):\n",
    "    return loss_fn(torch.tensor(landmarks[idx]), torch.tensor(dataset_landmark_to_pred(val_dataset[idx]))).numpy()\n",
    "\n",
    "def show_dataset_image(fn,ds):    \n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    \n",
    "#     dx = ds['crop_margin_x']\n",
    "#     dy = ds['crop_margin_y']\n",
    "    lm = dataset_landmark_to_pred(ds)\n",
    "    image = draw_landmarks(image, lm)    \n",
    "    plt.imshow(image)\n",
    "\n",
    "def show_predict_image(fn,lm):\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = draw_landmarks(image, lm)\n",
    "    plt.imshow(image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.load_state_dict(torch.load(f\"{prj_name}_best.pth\"))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Reading data...\")\n",
    "# поменяли трансформацию на валидационную, чтобы убрать рандом\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), val_transforms, split=\"train\",size = data_size) \n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=False,shuffle=False)# уберем перемешивание\n",
    "print(len(train_dataset))\n",
    "train_predictions = predict(model, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = predict(model, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочется сравнить разметку и результат работы модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = train_dataset\n",
    "landmarks = train_predictions\n",
    "max_len = len(dataset)\n",
    "print(max_len)\n",
    "max_err_idxs = []\n",
    "for i in range(0,max_len):\n",
    "    err = calc_err(i,landmarks,dataset,train_loss_fn)\n",
    "    if err>100:\n",
    "        max_err_idxs.append(i)\n",
    "        print(i,err)\n",
    "print(max_err_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = val_dataset\n",
    "landmarks = val_predictions\n",
    "max_len = len(dataset)\n",
    "print(max_len)\n",
    "max_err_idxs_val = []\n",
    "for i in range(0,max_len):\n",
    "    err = calc_err(i,landmarks,dataset,train_loss_fn)\n",
    "    if err>100:\n",
    "        max_err_idxs_val.append(i)\n",
    "        print(i,err)\n",
    "print(max_err_idxs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_image = set()\n",
    "for idx in max_err_idxs:\n",
    "    ignore_image.add(os.path.basename(train_dataset.image_names[idx]))\n",
    "    \n",
    "for idx in max_err_idxs_val:    \n",
    "    ignore_image.add(os.path.basename(val_dataset.image_names[idx]))\n",
    "print(len(ignore_image))\n",
    "with open(\"ignore_images_.lst\", \"wt\") as fp:\n",
    "    for s in ignore_image:    \n",
    "        print(s, file = fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES_TO_SHOW = 16\n",
    "NUM_COLS = 4\n",
    "NUM_ROWS = NUM_IMAGES_TO_SHOW // NUM_COLS + int(NUM_IMAGES_TO_SHOW % NUM_COLS != 0)\n",
    "\n",
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs_val[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_predict_image(val_dataset.image_names[idx],val_predictions[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs_val[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_dataset_image(val_dataset.image_names[idx],val_dataset[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_predict_image(train_dataset.image_names[idx],train_predictions[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_dataset_image(train_dataset.image_names[idx],train_dataset[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
