{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка данных\n",
    "Решил посмотреть как выглядят картинки на которых мы ошибаемся больше всего и увидел, что `train` содержит изображения со сдвинутой разметкой. \n",
    "\n",
    "Для их обнаружения обучается такая же модель, но на всех картинках, включая *испорченные*. После этого перебираются все изображения и делается на них предсказание. Если MSE больше 100 (просто константа), считаем, что на этой картинке неадекватная разметка.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "use_gpu = True\n",
    "data_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/_Data/full/\"\n",
    "learning_rate = 1e-3\n",
    "batch_size = 192\n",
    "epochs = 30\n",
    "prj_name = \"test\"\n",
    "# data_size = 40000\n",
    "# data_size = 5000\n",
    "# data_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метрика, которая учитывает масштабирование изображений\n",
    "class MseW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MseW,self).__init__()\n",
    "\n",
    "    def setWeight(self, weight):\n",
    "        self.w = weight\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        mse = torch.mul(outputs - labels,outputs - labels).mean(axis=1)        \n",
    "        mse=torch.mul(mse,self.w).mean(axis=0)\n",
    "        mse=mse.mean(axis=0)\n",
    "        mse=2*mse\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"training...\"):        \n",
    "        images = batch[\"image\"].to(device)  # B x 3 x CROP_SIZE x CROP_SIZE\n",
    "        landmarks = batch[\"landmarks\"]  # B x (2 * NUM_PTS)\n",
    "               \n",
    "        pred_landmarks = model(images).cpu()  # B x (2 * NUM_PTS)\n",
    "        loss = loss_fn(pred_landmarks, landmarks)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"validation...\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        landmarks = batch[\"landmarks\"]\n",
    "        coef = batch[\"scale_coef\"].numpy()\n",
    "        coef = 1/coef\n",
    "        coef = coef*coef\n",
    "        coef =torch.tensor(coef)\n",
    "        loss_fn.setWeight(coef)\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        loss = loss_fn(pred_landmarks, landmarks)\n",
    "        val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(loader.dataset), NUM_PTS, 2))\n",
    "    for i, batch in enumerate(tqdm.tqdm(loader, total=len(loader), desc=\"test prediction...\")):\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        pred_landmarks = pred_landmarks.numpy().reshape((len(pred_landmarks), NUM_PTS, 2))  # B x NUM_PTS x 2\n",
    "\n",
    "        fs = batch[\"scale_coef\"].numpy()  # B\n",
    "        margins_x = batch[\"crop_margin_x\"].numpy()  # B\n",
    "        margins_y = batch[\"crop_margin_y\"].numpy()  # B\n",
    "        if \"dx\" in batch.keys():\n",
    "            dx = batch[\"dx\"].numpy()  # B\n",
    "            dy = batch[\"dy\"].numpy()  # B\n",
    "            prediction = restore_landmarks_batch_ex(pred_landmarks, fs, margins_x, margins_y,dx,dy)  # B x NUM_PTS x 2\n",
    "        else:\n",
    "            prediction = restore_landmarks_batch(pred_landmarks, fs, margins_x, margins_y)  # B x NUM_PTS x 2\n",
    "        \n",
    "        predictions[i * loader.batch_size: (i + 1) * loader.batch_size] = prediction\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "NUM_PTS = 971\n",
    "CROP_SIZE = 128\n",
    "SUBMISSION_HEADER = \"file_name,Point_M0_X,Point_M0_Y,Point_M1_X,Point_M1_Y,Point_M2_X,Point_M2_Y,Point_M3_X,Point_M3_Y,Point_M4_X,Point_M4_Y,Point_M5_X,Point_M5_Y,Point_M6_X,Point_M6_Y,Point_M7_X,Point_M7_Y,Point_M8_X,Point_M8_Y,Point_M9_X,Point_M9_Y,Point_M10_X,Point_M10_Y,Point_M11_X,Point_M11_Y,Point_M12_X,Point_M12_Y,Point_M13_X,Point_M13_Y,Point_M14_X,Point_M14_Y,Point_M15_X,Point_M15_Y,Point_M16_X,Point_M16_Y,Point_M17_X,Point_M17_Y,Point_M18_X,Point_M18_Y,Point_M19_X,Point_M19_Y,Point_M20_X,Point_M20_Y,Point_M21_X,Point_M21_Y,Point_M22_X,Point_M22_Y,Point_M23_X,Point_M23_Y,Point_M24_X,Point_M24_Y,Point_M25_X,Point_M25_Y,Point_M26_X,Point_M26_Y,Point_M27_X,Point_M27_Y,Point_M28_X,Point_M28_Y,Point_M29_X,Point_M29_Y\\n\"\n",
    "   \n",
    "class CropRandom(object):\n",
    "    def __init__(self, size=CROP_SIZE, elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if 'landmarks' in sample:\n",
    "            img = sample[self.elem_name] #.copy()\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            bound=landmarks[:,0].min(),landmarks[:,1].min(),landmarks[:,0].max(),landmarks[:,1].max()                        \n",
    "            h,w,_ = img.shape\n",
    "            min_sq = max(bound[3]-bound[1],bound[2]-bound[0])\n",
    "            max_sq = min(w,h)            \n",
    "            if min_sq+1<max_sq-1:\n",
    "                sq = np.random.randint(min_sq+1,max_sq-1)\n",
    "            else:\n",
    "                sq = max_sq-1                \n",
    "            \n",
    "            min_dx = max(bound[2]-sq,0)\n",
    "            max_dx = min(w-sq,bound[0])\n",
    "            if min_dx<max_dx:\n",
    "                dx = np.random.randint(min_dx,max_dx)\n",
    "            else:\n",
    "                dx = int(min_dx)\n",
    "            \n",
    "            min_dy = max(bound[3]-sq,0)\n",
    "            max_dy = min(h-sq,bound[1])\n",
    "            if min_dy<max_dy:\n",
    "                dy = np.random.randint(min_dy,max_dy)\n",
    "            else:                \n",
    "                dy = int(min_dy)\n",
    "                \n",
    "            landmarks -= torch.tensor((dx, dy), dtype=landmarks.dtype)[None, :]                        \n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "            sample[self.elem_name] = img[dy:dy+sq, dx:dx+sq]\n",
    "            sample['dx'] = torch.tensor(dx,dtype=torch.short)\n",
    "            sample['dy'] = torch.tensor(dy,dtype=torch.short)\n",
    "        else:\n",
    "            raise RuntimeError(f\"stop\")\n",
    "            sample['dx'] = torch.tensor(0)\n",
    "            sample['dy'] = torch.tensor(0)\n",
    "        return sample    \n",
    "    \n",
    "\n",
    "class ScaleMinSideToSize(object):\n",
    "    def __init__(self, size=(CROP_SIZE, CROP_SIZE), elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w, _ = sample[self.elem_name].shape        \n",
    "        if h > w:\n",
    "            f = self.size[0] / w\n",
    "        else:\n",
    "            f = self.size[1] / h\n",
    "\n",
    "        sample[self.elem_name] = cv2.resize(sample[self.elem_name], None, fx=f, fy=f, interpolation=cv2.INTER_AREA)\n",
    "        sample[\"scale_coef\"] = f\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2).float()\n",
    "            landmarks = landmarks * f\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CropCenter(object):\n",
    "    def __init__(self, size=128, elem_name='image'):\n",
    "        self.size = size\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample[self.elem_name]\n",
    "        h, w, _ = img.shape\n",
    "        margin_h = (h - self.size) // 2\n",
    "        margin_w = (w - self.size) // 2\n",
    "        sample[self.elem_name] = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n",
    "        sample[\"crop_margin_x\"] = margin_w\n",
    "        sample[\"crop_margin_y\"] = margin_h\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            landmarks -= torch.tensor((margin_w, margin_h), dtype=landmarks.dtype)[None, :]\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "# class RandomFlipV(object):\n",
    "#     def __init__(self, size=128, elem_name='image'):\n",
    "#         self.size = size\n",
    "#         self.elem_name = elem_name\n",
    "\n",
    "#     def __call__(self, sample):\n",
    "#         if np.random.randint(0,10)>4:\n",
    "#             sample['flip'] = True                    \n",
    "#             img = sample[self.elem_name]            \n",
    "#             sample[self.elem_name] = img[:,::-1,:]\n",
    "#             if 'landmarks' in sample:\n",
    "#                 landmarks = sample['flip_landmarks']                \n",
    "#                 landmarks[:,0] = img.shape[1]-landmarks[:,0] \n",
    "#                 sample['landmarks'] = landmarks #torch.as_tensor(flip_lm_v(landmarks,img.shape))\n",
    "#         else:\n",
    "#             sample['flip'] = False\n",
    "#         return sample \n",
    "    \n",
    "class TransformByKeys(object):\n",
    "    def __init__(self, transform, names):\n",
    "        self.transform = transform\n",
    "        self.names = set(names)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for name in self.names:\n",
    "            if name in sample:\n",
    "                sample[name] = self.transform(sample[name])\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ThousandLandmarksDataset(data.Dataset):\n",
    "    def __init__(self, root, transforms, split=\"train\", size=None, ignore_image=None): # , isFlip=False\n",
    "        super(ThousandLandmarksDataset, self).__init__()\n",
    "        self.root = root\n",
    "        landmark_file_name = os.path.join(root, 'landmarks.csv') if split != \"test\" \\\n",
    "            else os.path.join(root, \"test_points.csv\")\n",
    "        images_root = os.path.join(root, \"images\")\n",
    "\n",
    "        self.image_names = []\n",
    "        self.landmarks = []\n",
    "#         self.flip_landmarks = []\n",
    "\n",
    "        if size is None:            \n",
    "            with open(landmark_file_name, \"rt\") as fp:\n",
    "                num_lines = sum(1 for line in fp)\n",
    "        else:\n",
    "            num_lines = size-1\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            for i, line in tqdm.tqdm(enumerate(fp)):\n",
    "                if i == 0:\n",
    "                    continue  # skip header\n",
    "                if split == \"train\" and i == int(TRAIN_SIZE * num_lines):\n",
    "                    break  # reached end of train part of data\n",
    "                elif split == \"val\" and i < int(TRAIN_SIZE * num_lines):\n",
    "                    continue  # has not reached start of val part of data\n",
    "                if i>=int(num_lines):\n",
    "                    break # чтобы можно было грузить меньше картинок\n",
    "                elements = line.strip().split(\"\\t\")\n",
    "                if ignore_image is not None and elements[0] in ignore_image:\n",
    "                    print(\"ignore: \",elements[0])\n",
    "                    continue\n",
    "                    \n",
    "                image_name = os.path.join(images_root, elements[0])                \n",
    "                self.image_names.append(image_name)            \n",
    "                \n",
    "                if split in (\"train\", \"val\"):\n",
    "                    landmarks = list(map(np.int16, elements[1:]))\n",
    "                    landmarks = np.array(landmarks, dtype=np.int16).reshape((len(landmarks) // 2, 2))                                                            \n",
    "                    self.landmarks.append(landmarks)\n",
    "#                     flip_landmarks = flip_lm_v(landmarks)\n",
    "#                     self.flip_landmarks.append(flip_landmarks)\n",
    "\n",
    "        if split in (\"train\", \"val\"):\n",
    "            self.landmarks = torch.as_tensor(self.landmarks)\n",
    "            self.flip_landmarks = torch.as_tensor(self.flip_landmarks)\n",
    "        else:\n",
    "            self.landmarks = None\n",
    "            self.flip_landmarks = None\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "\n",
    "        image = cv2.imread(self.image_names[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                \n",
    "        sample[\"image\"] = image\n",
    "        \n",
    "        if self.landmarks is not None:\n",
    "            landmarks = self.landmarks[idx].clone()\n",
    "#             flip_landmarks = self.flip_landmarks[idx].clone()\n",
    "            \n",
    "            sample[\"landmarks\"] = landmarks\n",
    "#             sample[\"flip_landmarks\"] = flip_landmarks\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "def restore_landmarks_batch_ex(landmarks, fs, margins_x, margins_y,dx,dy):\n",
    "    landmarks[:, :, 0] += margins_x[:, None]\n",
    "    landmarks[:, :, 1] += margins_y[:, None]\n",
    "    landmarks /= fs[:, None, None]\n",
    "    landmarks[:, :, 0] += dx[:, None]\n",
    "    landmarks[:, :, 1] += dy[:, None]\n",
    "    return landmarks\n",
    "def restore_landmarks_batch(landmarks, fs, margins_x, margins_y):\n",
    "    landmarks[:, :, 0] += margins_x[:, None]\n",
    "    landmarks[:, :, 1] += margins_y[:, None]\n",
    "    landmarks /= fs[:, None, None]    \n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def create_submission(path_to_data, test_predictions, path_to_submission_file):\n",
    "    test_dir = os.path.join(path_to_data, \"test\")\n",
    "\n",
    "    output_file = path_to_submission_file\n",
    "    wf = open(output_file, 'w')\n",
    "    wf.write(SUBMISSION_HEADER)\n",
    "\n",
    "    mapping_path = os.path.join(test_dir, 'test_points.csv')\n",
    "    mapping = pd.read_csv(mapping_path, delimiter='\\t')\n",
    "\n",
    "    for i, row in mapping.iterrows():\n",
    "        file_name = row[0]\n",
    "        point_index_list = np.array(eval(row[1]))\n",
    "        points_for_image = test_predictions[i]\n",
    "        needed_points = points_for_image[point_index_list].astype(np.int)\n",
    "        wf.write(file_name + ',' + ','.join(map(str, needed_points.reshape(2 * len(point_index_list)))) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, landmarks):\n",
    "    for point in landmarks:\n",
    "        x, y = point.astype(np.int)\n",
    "        cv2.circle(image, (x, y), 1, (128, 0, 128), 1, -1)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fn = fnn.mse_loss\n",
    "valid_loss_fn = MseW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline при обучении\n",
    "train_transforms = transforms.Compose([\n",
    "#     RandomFlipV(),\n",
    "    CropRandom(),    \n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),   \n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])\n",
    "# pipeline при валидации и предсказании\n",
    "val_transforms = transforms.Compose([\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Reading data...\")\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), train_transforms, split=\"train\",size = data_size) \n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=True,\n",
    "                                   shuffle=True)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), val_transforms, split=\"val\",size = data_size)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=False,shuffle=False)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\")\n",
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. train & validate\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(0,epochs):    \n",
    "    train_loss = train(model, train_dataloader, train_loss_fn, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, valid_loss_fn, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.4f}\\tval loss: {:5.4f}\".format(epoch, train_loss, val_loss))    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"{prj_name}_best.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "    with open(f\"{prj_name}_\"+str(epoch)+\".pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предскажем значения для валидационного множества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_landmark_to_pred(ds):\n",
    "    lm = ds['landmarks'].numpy().copy()\n",
    "    dx = ds['crop_margin_x']\n",
    "    dy = ds['crop_margin_y']    \n",
    "    for ix in range(0,len(lm),2):\n",
    "        lm[ix]+=dx\n",
    "    for iy in range(1,len(lm),2):\n",
    "        lm[iy]+=dy\n",
    "    coef = ds['scale_coef'].numpy()    \n",
    "    lm=lm/coef\n",
    "    lm = lm.reshape(-1,2)\n",
    "    return lm\n",
    "\n",
    "def calc_err(idx,landmarks,val_dataset,loss_fn):\n",
    "    return loss_fn(torch.tensor(landmarks[idx]), torch.tensor(dataset_landmark_to_pred(val_dataset[idx]))).numpy()\n",
    "\n",
    "def show_dataset_image(fn,ds):    \n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    \n",
    "#     dx = ds['crop_margin_x']\n",
    "#     dy = ds['crop_margin_y']\n",
    "    lm = dataset_landmark_to_pred(ds)\n",
    "    image = draw_landmarks(image, lm)    \n",
    "    plt.imshow(image)\n",
    "\n",
    "def show_predict_image(fn,lm):\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = draw_landmarks(image, lm)\n",
    "    plt.imshow(image)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.load_state_dict(torch.load(f\"{prj_name}_best.pth\"))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Reading data...\")\n",
    "# поменяли трансформацию на валидационную, чтобы убрать рандом\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), val_transforms, split=\"train\",size = data_size) \n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=False,shuffle=False)# уберем перемешивание\n",
    "print(len(train_dataset))\n",
    "train_predictions = predict(model, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = predict(model, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочется сравнить разметку и результат работы модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = train_dataset\n",
    "landmarks = train_predictions\n",
    "max_len = len(dataset)\n",
    "print(max_len)\n",
    "max_err_idxs = []\n",
    "for i in range(0,max_len):\n",
    "    err = calc_err(i,landmarks,dataset,train_loss_fn)\n",
    "    if err>100:\n",
    "        max_err_idxs.append(i)\n",
    "        print(i,err)\n",
    "print(max_err_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = val_dataset\n",
    "landmarks = val_predictions\n",
    "max_len = len(dataset)\n",
    "print(max_len)\n",
    "max_err_idxs_val = []\n",
    "for i in range(0,max_len):\n",
    "    err = calc_err(i,landmarks,dataset,train_loss_fn)\n",
    "    if err>100:\n",
    "        max_err_idxs_val.append(i)\n",
    "        print(i,err)\n",
    "print(max_err_idxs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_image = set()\n",
    "for idx in max_err_idxs:\n",
    "    ignore_image.add(os.path.basename(train_dataset.image_names[idx]))\n",
    "    \n",
    "for idx in max_err_idxs_val:    \n",
    "    ignore_image.add(os.path.basename(val_dataset.image_names[idx]))\n",
    "print(len(ignore_image))\n",
    "with open(\"ignore_images_.lst\", \"wt\") as fp:\n",
    "    for s in ignore_image:    \n",
    "        print(s, file = fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES_TO_SHOW = 16\n",
    "NUM_COLS = 4\n",
    "NUM_ROWS = NUM_IMAGES_TO_SHOW // NUM_COLS + int(NUM_IMAGES_TO_SHOW % NUM_COLS != 0)\n",
    "\n",
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs_val[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_predict_image(val_dataset.image_names[idx],val_predictions[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs_val[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_dataset_image(val_dataset.image_names[idx],val_dataset[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_predict_image(train_dataset.image_names[idx],train_predictions[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, NUM_ROWS * 8))\n",
    "for i, idx in enumerate(max_err_idxs[:16], 1):    \n",
    "    plt.subplot(NUM_ROWS, NUM_COLS, i)\n",
    "    show_dataset_image(train_dataset.image_names[idx],train_dataset[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
