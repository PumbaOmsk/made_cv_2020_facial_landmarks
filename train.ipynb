{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Выкинул ~3000 изображений, со смещенной разметкой. Можно было им сдвинуть разметку в правильное место и тоже использовать, но решил, что их всего 1%.\n",
    "1. Аугментация - RandomCrop - случайным образом выбирал размер квадрата, который больше landmark и меньше, чем картинка. И случайным образом смещал его относительно разметки, но чтобы она осталась в нём. (такая была только на обучении, на валидации и тесте - из бейзлайна)\n",
    "2. AdamW\n",
    "3. обучался на MSE из бейзлайна, а на валидации метрику пересчитывал с учётом трансформации.\n",
    "4. Эпох = пока падали метрики ~70\n",
    "5. ResNet50, не морозил.\n",
    "\n",
    "Не делал или не зашло:\n",
    "С цветами, яркостью и блюром почему-то не стал играть. Может и зря.  \n",
    "18-34-50 - давали улучшения. ResNet101 - как-то не пошёл. Пробовал другие предобученые модели - не было той, чтобы понравилось. Искал что-нибудь, чтобы быстрее училась.  \n",
    "Пробовал Flip начальной картинки - для этого сделал корректный пересчёт точек, но сначала сделал все медленно и как-то не давало улучшения, потом переделал чтобы быстро работало. Явных улучшений не было, но были странные расшатывания модели. Когда на очередной итерации скор мог скакнуть сильно.  \n",
    "Хотел как-нибудь использовать keypoint - но предобученая модель давала странный результат (находила на 1 лице несколько комплектов точек) и забросил.  \n",
    "Пробовал в конце 2 FC слоя с дропаутом - не понравилось.  \n",
    "Пробовал учить только 1 последний слой - тоже не зашло.  \n",
    "Была мысль, что может зря выкидываю изображения со смещенной разметкой при обучении. Вдруг в тесте тоже такие будут. Но потом решил, что там такой дикий скор получается на них, что нет смысла. Да и задача у нас все-таки правильную разметку строить, а не угадывать когда она сдвинута."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils import NUM_PTS, CROP_SIZE\n",
    "from utils import train,validate,predict\n",
    "from utils import ScaleMinSideToSize,CropCenter,TransformByKeys\n",
    "from utils import ThousandLandmarksDataset\n",
    "from utils import restore_landmarks_batch, create_submission,restore_landmarks_batch_ex,draw_landmarks\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "use_gpu = True\n",
    "data_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_ignore = \"ignore_images.lst\"\n",
    "data_dir = \"C:/_Data/full/\"\n",
    "batch_size = 192\n",
    "learning_rate = 1e-3\n",
    "epochs1 = 30\n",
    "epochs2 = 40\n",
    "prj_name = \"test10\"\n",
    "# data_size = 40000\n",
    "# data_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метрика, которая учитывает масштабирование изображений - для отображения при валидации\n",
    "class MseW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MseW,self).__init__()\n",
    "\n",
    "    def setWeight(self, weight):\n",
    "        self.w = weight\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        mse = torch.mul(outputs - labels,outputs - labels).mean(axis=1)        \n",
    "        mse=torch.mul(mse,self.w).mean(axis=0)\n",
    "        mse=mse.mean(axis=0)\n",
    "        mse=2*mse\n",
    "        return mse        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropRandom(object):\n",
    "    def __init__(self, size=CROP_SIZE, elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if 'landmarks' in sample:\n",
    "            img = sample[self.elem_name] #.copy()\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            bound=landmarks[:,0].min(),landmarks[:,1].min(),landmarks[:,0].max(),landmarks[:,1].max()                        \n",
    "            h,w,_ = img.shape\n",
    "            min_sq = max(bound[3]-bound[1],bound[2]-bound[0])\n",
    "            max_sq = min(w,h)            \n",
    "            if min_sq+1<max_sq-1:\n",
    "                sq = np.random.randint(min_sq+1,max_sq-1)\n",
    "            else:\n",
    "                sq = max_sq-1                \n",
    "            \n",
    "            min_dx = max(bound[2]-sq,0)\n",
    "            max_dx = min(w-sq,bound[0])\n",
    "            if min_dx<max_dx:\n",
    "                dx = np.random.randint(min_dx,max_dx)\n",
    "            else:\n",
    "                dx = int(min_dx)\n",
    "            \n",
    "            min_dy = max(bound[3]-sq,0)\n",
    "            max_dy = min(h-sq,bound[1])\n",
    "            if min_dy<max_dy:\n",
    "                dy = np.random.randint(min_dy,max_dy)\n",
    "            else:                \n",
    "                dy = int(min_dy)\n",
    "                \n",
    "            landmarks -= torch.tensor((dx, dy), dtype=landmarks.dtype)[None, :]                        \n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "            sample[self.elem_name] = img[dy:dy+sq, dx:dx+sq]\n",
    "            sample['dx'] = torch.tensor(dx,dtype=torch.short)\n",
    "            sample['dy'] = torch.tensor(dy,dtype=torch.short)\n",
    "        else:\n",
    "            raise RuntimeError(f\"stop\")\n",
    "            sample['dx'] = torch.tensor(0)\n",
    "            sample['dy'] = torch.tensor(0)\n",
    "        return sample    \n",
    "       \n",
    "# class RandomFlipV(object):\n",
    "#     def __init__(self, size=128, elem_name='image'):\n",
    "#         self.size = size\n",
    "#         self.elem_name = elem_name\n",
    "\n",
    "#     def __call__(self, sample):\n",
    "#         if np.random.randint(0,10)>4:\n",
    "#             sample['flip'] = True                    \n",
    "#             img = sample[self.elem_name]            \n",
    "#             sample[self.elem_name] = img[:,::-1,:]\n",
    "#             if 'landmarks' in sample:\n",
    "#                 landmarks = sample['flip_landmarks']                \n",
    "#                 landmarks[:,0] = img.shape[1]-landmarks[:,0] \n",
    "#                 sample['landmarks'] = landmarks #torch.as_tensor(flip_lm_v(landmarks,img.shape))\n",
    "#         else:\n",
    "#             sample['flip'] = False\n",
    "#         return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_image = set()\n",
    "with open(fn_ignore, \"rt\") as fp:\n",
    "    for line in fp:\n",
    "        ignore_image.add(line.strip())\n",
    "print(len(ignore_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fn = fnn.mse_loss\n",
    "valid_loss_fn = MseW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline при обучении\n",
    "train_transforms = transforms.Compose([\n",
    "#     RandomFlipV(),\n",
    "    CropRandom(),    \n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])\n",
    "# pipeline при валидации и предсказании\n",
    "val_transforms = transforms.Compose([\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Reading data...\")\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), train_transforms, split=\"train\",size = data_size,ignore_image=ignore_image)\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=True,shuffle=True)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), val_transforms, split=\"val\",size = data_size,ignore_image=ignore_image)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=False,shuffle=False)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\")\n",
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. train & validate\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(0,epochs1):    \n",
    "    train_loss = train(model, train_dataloader, train_loss_fn, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, valid_loss_fn, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.4f}\\tval loss: {:5.4f}\".format(epoch, train_loss, val_loss))    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"{prj_name}_best.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "    with open(f\"{prj_name}_\"+str(epoch)+\".pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. train & validate\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(epochs1,epochs1+epochs2):    \n",
    "    train_loss = train(model, train_dataloader, train_loss_fn, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, valid_loss_fn, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.4f}\\tval loss: {:5.4f}\".format(epoch, train_loss, val_loss))    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"{prj_name}_best.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "    with open(f\"{prj_name}_\"+str(epoch)+\".pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. predict last\n",
    "test_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'test'), val_transforms, split=\"test\")\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,\n",
    "                                  shuffle=False, drop_last=False)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n",
    "with open(f\"{prj_name}_last_test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                 \"landmarks\": test_predictions}, fp)\n",
    "\n",
    "create_submission(data_dir, test_predictions, f\"{prj_name}_last_submit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict best\n",
    "with open(f\"{prj_name}_best.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n",
    "with open(f\"{prj_name}_best_test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                 \"landmarks\": test_predictions}, fp)\n",
    "\n",
    "create_submission(data_dir, test_predictions, f\"{prj_name}_best_submit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
