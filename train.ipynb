{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Выкинул ~3000 изображений, со смещенной разметкой. Можно было им сдвинуть разметку в правильное место и тоже использовать, но решил, что их всего 1%.\n",
    "1. Аугментация - RandomCrop - случайным образом выбирал размер квадрата, который больше landmark и меньше, чем картинка. И случайным образом смещал его относительно разметки, но чтобы она осталась в нём. (такая была только на обучении, на валидации и тесте - из бейзлайна)\n",
    "2. AdamW\n",
    "3. обучался на MSE из бейзлайна, а на валидации метрику пересчитывал с учётом трансформации.\n",
    "4. Эпох = пока падали метрики ~70\n",
    "5. ResNet50, не морозил.\n",
    "\n",
    "Не делал или не зашло:\n",
    "С цветами, яркостью и блюром почему-то не стал играть. Может и зря.  \n",
    "18-34-50 - давали улучшения. ResNet101 - как-то не пошёл. Пробовал другие предобученые модели - не было той, чтобы понравилось. Искал что-нибудь, чтобы быстрее училась.  \n",
    "Пробовал Flip начальной картинки - для этого сделал корректный пересчёт точек, но сначала сделал все медленно и как-то не давало улучшения, потом переделал чтобы быстро работало. Явных улучшений не было, но были странные расшатывания модели. Когда на очередной итерации скор мог скакнуть сильно.  \n",
    "Хотел как-нибудь использовать keypoint - но предобученая модель давала странный результат (находила на 1 лице несколько комплектов точек) и забросил.  \n",
    "Пробовал в конце 2 FC слоя с дропаутом - не понравилось.  \n",
    "Пробовал учить только 1 последний слой - тоже не зашло.  \n",
    "Была мысль, что может зря выкидываю изображения со смещенной разметкой при обучении. Вдруг в тесте тоже такие будут. Но потом решил, что там такой дикий скор получается на них, что нет смысла. Да и задача у нас все-таки правильную разметку строить, а не угадывать когда она сдвинута."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16e0023c450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "use_gpu = True\n",
    "data_size = None\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_ignore = \"ignore_images.lst\"\n",
    "data_dir = \"C:/_Data/full/\"\n",
    "batch_size = 192\n",
    "learning_rate = 1e-3\n",
    "epochs1 = 30\n",
    "epochs2 = 40\n",
    "prj_name = \"test10\"\n",
    "# data_size = 40000\n",
    "data_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функции отражения разметки. В итоговой модели не пригодились\n",
    "# def flip_lm_width(lm,shape):\n",
    "#     lmtr = lm.copy()    \n",
    "#     height, width, _ = shape        \n",
    "#     lmtr[:,0] = width-lmtr[:,0]    \n",
    "        \n",
    "#     lmf = np.zeros(len(lm)*2).reshape(-1,2);            \n",
    "#     lmf[0:64] = lmtr[64:128]  # овал лица лево низ\n",
    "#     lmf[64:128]= lmtr[0:64]   # овал лица право низ\n",
    "#     lmf[128:200]= lmtr[272:200:-1] # овал лица право верх\n",
    "#     lmf[200] = lmtr[200] # центр на лбу\n",
    "#     lmf[201:273]= lmtr[199:127:-1] # овал лица лево верх\n",
    "#     lmf[273:273+64]= lmtr[337:337+64] # левая бровь \n",
    "#     lmf[337:337+64]= lmtr[273:273+64] # правая бровь    \n",
    "#     lmf[401:401+63]= lmtr[464:464+63] # левая душка носа\n",
    "#     lmf[464:464+63]= lmtr[401:401+63] # правая душка носа\n",
    "#     lmf[527:527+60]= lmtr[527:527+60] # центр носа\n",
    "#     lmf[587:587+63]= lmtr[714:714+63] # левый зрачок\n",
    "#     lmf[714:714+63]= lmtr[587:587+63] # правый зрачок\n",
    "#     lmf[650:650+64]= lmtr[777:777+64] # левый глаз\n",
    "#     lmf[777:777+64]= lmtr[650:650+64] # правый глаз\n",
    "#     lmf[841:841+32]= lmtr[840+32:840:-1] # рот верх \n",
    "#     lmf[873:873+32]= lmtr[872+32:872:-1] # рот верх зубы\n",
    "#     lmf[905:905+32]= lmtr[904+32:904:-1] # рот низ \n",
    "#     lmf[937:937+32]= lmtr[936+32:936:-1] # рот низ зубы\n",
    "#     lmf[969] = lmtr[970] # зрачок левый\n",
    "#     lmf[970] = lmtr[969] # зрачок правый\n",
    "    \n",
    "#     return lmf\n",
    "\n",
    "# def flip_lm_v(lm):\n",
    "#     lmtr = lm.copy()\n",
    "#     lmf = np.zeros(len(lmtr)*2).reshape(-1,2);        \n",
    "#     lmf[0:64] = lmtr[64:128]  # овал лица лево низ\n",
    "#     lmf[64:128]= lmtr[0:64]   # овал лица право низ\n",
    "#     lmf[128:200]= lmtr[272:200:-1] # овал лица право верх\n",
    "#     lmf[200] = lmtr[200] # центр на лбу\n",
    "#     lmf[201:273]= lmtr[199:127:-1] # овал лица лево верх\n",
    "#     lmf[273:273+64]= lmtr[337:337+64] # левая бровь \n",
    "#     lmf[337:337+64]= lmtr[273:273+64] # правая бровь    \n",
    "#     lmf[401:401+63]= lmtr[464:464+63] # левая душка носа\n",
    "#     lmf[464:464+63]= lmtr[401:401+63] # правая душка носа\n",
    "#     lmf[527:527+60]= lmtr[527:527+60] # центр носа\n",
    "#     lmf[587:587+63]= lmtr[714:714+63] # левый зрачок\n",
    "#     lmf[714:714+63]= lmtr[587:587+63] # правый зрачок\n",
    "#     lmf[650:650+64]= lmtr[777:777+64] # левый глаз\n",
    "#     lmf[777:777+64]= lmtr[650:650+64] # правый глаз\n",
    "#     lmf[841:841+32]= lmtr[840+32:840:-1] # рот верх \n",
    "#     lmf[873:873+32]= lmtr[872+32:872:-1] # рот верх зубы\n",
    "#     lmf[905:905+32]= lmtr[904+32:904:-1] # рот низ \n",
    "#     lmf[937:937+32]= lmtr[936+32:936:-1] # рот низ зубы\n",
    "#     lmf[969] = lmtr[970] # зрачок левый\n",
    "#     lmf[970] = lmtr[969] # зрачок правый\n",
    "    \n",
    "#     return lmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метрика, которая учитывает масштабирование изображений - для отображения при валидации\n",
    "class MseW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MseW,self).__init__()\n",
    "\n",
    "    def setWeight(self, weight):\n",
    "        self.w = weight\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        mse = torch.mul(outputs - labels,outputs - labels).mean(axis=1)        \n",
    "        mse=torch.mul(mse,self.w).mean(axis=0)\n",
    "        mse=mse.mean(axis=0)\n",
    "        mse=2*mse\n",
    "        return mse        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"training...\"):        \n",
    "        images = batch[\"image\"].to(device)  # B x 3 x CROP_SIZE x CROP_SIZE\n",
    "        landmarks = batch[\"landmarks\"]  # B x (2 * NUM_PTS)                \n",
    "        pred_landmarks = model(images).cpu()  # B x (2 * NUM_PTS)\n",
    "        loss = loss_fn(pred_landmarks, landmarks)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"validation...\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        landmarks = batch[\"landmarks\"]\n",
    "        coef = batch[\"scale_coef\"].numpy()\n",
    "        coef = 1/coef\n",
    "        coef = coef*coef\n",
    "        coef =torch.tensor(coef)\n",
    "        loss_fn.setWeight(coef)\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        loss = loss_fn(pred_landmarks, landmarks)\n",
    "        val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(loader.dataset), NUM_PTS, 2))\n",
    "    for i, batch in enumerate(tqdm.tqdm(loader, total=len(loader), desc=\"test prediction...\")):\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        pred_landmarks = pred_landmarks.numpy().reshape((len(pred_landmarks), NUM_PTS, 2))  # B x NUM_PTS x 2\n",
    "\n",
    "        fs = batch[\"scale_coef\"].numpy()  # B\n",
    "        margins_x = batch[\"crop_margin_x\"].numpy()  # B\n",
    "        margins_y = batch[\"crop_margin_y\"].numpy()  # B\n",
    "        if \"dx\" in batch.keys():\n",
    "            dx = batch[\"dx\"].numpy()  # B\n",
    "            dy = batch[\"dy\"].numpy()  # B\n",
    "            prediction = restore_landmarks_batch_ex(pred_landmarks, fs, margins_x, margins_y,dx,dy)  # B x NUM_PTS x 2\n",
    "        else:\n",
    "            prediction = restore_landmarks_batch(pred_landmarks, fs, margins_x, margins_y)  # B x NUM_PTS x 2\n",
    "        \n",
    "        predictions[i * loader.batch_size: (i + 1) * loader.batch_size] = prediction\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "NUM_PTS = 971\n",
    "CROP_SIZE = 128\n",
    "SUBMISSION_HEADER = \"file_name,Point_M0_X,Point_M0_Y,Point_M1_X,Point_M1_Y,Point_M2_X,Point_M2_Y,Point_M3_X,Point_M3_Y,Point_M4_X,Point_M4_Y,Point_M5_X,Point_M5_Y,Point_M6_X,Point_M6_Y,Point_M7_X,Point_M7_Y,Point_M8_X,Point_M8_Y,Point_M9_X,Point_M9_Y,Point_M10_X,Point_M10_Y,Point_M11_X,Point_M11_Y,Point_M12_X,Point_M12_Y,Point_M13_X,Point_M13_Y,Point_M14_X,Point_M14_Y,Point_M15_X,Point_M15_Y,Point_M16_X,Point_M16_Y,Point_M17_X,Point_M17_Y,Point_M18_X,Point_M18_Y,Point_M19_X,Point_M19_Y,Point_M20_X,Point_M20_Y,Point_M21_X,Point_M21_Y,Point_M22_X,Point_M22_Y,Point_M23_X,Point_M23_Y,Point_M24_X,Point_M24_Y,Point_M25_X,Point_M25_Y,Point_M26_X,Point_M26_Y,Point_M27_X,Point_M27_Y,Point_M28_X,Point_M28_Y,Point_M29_X,Point_M29_Y\\n\"\n",
    "\n",
    "class CropRandom(object):\n",
    "    def __init__(self, size=CROP_SIZE, elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if 'landmarks' in sample:\n",
    "            img = sample[self.elem_name] #.copy()\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            bound=landmarks[:,0].min(),landmarks[:,1].min(),landmarks[:,0].max(),landmarks[:,1].max()                        \n",
    "            h,w,_ = img.shape\n",
    "            min_sq = max(bound[3]-bound[1],bound[2]-bound[0])\n",
    "            max_sq = min(w,h)            \n",
    "            if min_sq+1<max_sq-1:\n",
    "                sq = np.random.randint(min_sq+1,max_sq-1)\n",
    "            else:\n",
    "                sq = max_sq-1                \n",
    "            \n",
    "            min_dx = max(bound[2]-sq,0)\n",
    "            max_dx = min(w-sq,bound[0])\n",
    "            if min_dx<max_dx:\n",
    "                dx = np.random.randint(min_dx,max_dx)\n",
    "            else:\n",
    "                dx = int(min_dx)\n",
    "            \n",
    "            min_dy = max(bound[3]-sq,0)\n",
    "            max_dy = min(h-sq,bound[1])\n",
    "            if min_dy<max_dy:\n",
    "                dy = np.random.randint(min_dy,max_dy)\n",
    "            else:                \n",
    "                dy = int(min_dy)\n",
    "                \n",
    "            landmarks -= torch.tensor((dx, dy), dtype=landmarks.dtype)[None, :]                        \n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "            sample[self.elem_name] = img[dy:dy+sq, dx:dx+sq]\n",
    "            sample['dx'] = torch.tensor(dx,dtype=torch.short)\n",
    "            sample['dy'] = torch.tensor(dy,dtype=torch.short)\n",
    "        else:\n",
    "            raise RuntimeError(f\"stop\")\n",
    "            sample['dx'] = torch.tensor(0)\n",
    "            sample['dy'] = torch.tensor(0)\n",
    "        return sample    \n",
    "    \n",
    "\n",
    "class ScaleMinSideToSize(object):\n",
    "    def __init__(self, size=(CROP_SIZE, CROP_SIZE), elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w, _ = sample[self.elem_name].shape        \n",
    "        if h > w:\n",
    "            f = self.size[0] / w\n",
    "        else:\n",
    "            f = self.size[1] / h\n",
    "\n",
    "        sample[self.elem_name] = cv2.resize(sample[self.elem_name], None, fx=f, fy=f, interpolation=cv2.INTER_AREA)\n",
    "        sample[\"scale_coef\"] = f\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2).float()\n",
    "            landmarks = landmarks * f\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CropCenter(object):\n",
    "    def __init__(self, size=128, elem_name='image'):\n",
    "        self.size = size\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample[self.elem_name]\n",
    "        h, w, _ = img.shape\n",
    "        margin_h = (h - self.size) // 2\n",
    "        margin_w = (w - self.size) // 2\n",
    "        sample[self.elem_name] = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n",
    "        sample[\"crop_margin_x\"] = margin_w\n",
    "        sample[\"crop_margin_y\"] = margin_h\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            landmarks -= torch.tensor((margin_w, margin_h), dtype=landmarks.dtype)[None, :]\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class RandomFlipV(object):\n",
    "    def __init__(self, size=128, elem_name='image'):\n",
    "        self.size = size\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if np.random.randint(0,10)>4:\n",
    "            sample['flip'] = True                    \n",
    "            img = sample[self.elem_name]            \n",
    "            sample[self.elem_name] = img[:,::-1,:]\n",
    "            if 'landmarks' in sample:\n",
    "                landmarks = sample['flip_landmarks']                \n",
    "                landmarks[:,0] = img.shape[1]-landmarks[:,0] \n",
    "                sample['landmarks'] = landmarks #torch.as_tensor(flip_lm_v(landmarks,img.shape))\n",
    "        else:\n",
    "            sample['flip'] = False\n",
    "        return sample \n",
    "    \n",
    "class TransformByKeys(object):\n",
    "    def __init__(self, transform, names):\n",
    "        self.transform = transform\n",
    "        self.names = set(names)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for name in self.names:\n",
    "            if name in sample:\n",
    "                sample[name] = self.transform(sample[name])\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ThousandLandmarksDataset(data.Dataset):\n",
    "    def __init__(self, root, transforms, split=\"train\", size=None, ignore_image=None): # , isFlip=False\n",
    "        super(ThousandLandmarksDataset, self).__init__()\n",
    "        self.root = root\n",
    "        landmark_file_name = os.path.join(root, 'landmarks.csv') if split != \"test\" \\\n",
    "            else os.path.join(root, \"test_points.csv\")\n",
    "        images_root = os.path.join(root, \"images\")\n",
    "\n",
    "        self.image_names = []\n",
    "        self.landmarks = []\n",
    "#         self.flip_landmarks = []\n",
    "\n",
    "        if size is None:            \n",
    "            with open(landmark_file_name, \"rt\") as fp:\n",
    "                num_lines = sum(1 for line in fp)\n",
    "#             num_lines -= 1  # header\n",
    "        else:\n",
    "            num_lines = size-1\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            for i, line in tqdm.tqdm(enumerate(fp)):\n",
    "                if i == 0:\n",
    "                    continue  # skip header\n",
    "                if split == \"train\" and i == int(TRAIN_SIZE * num_lines):\n",
    "                    break  # reached end of train part of data\n",
    "                elif split == \"val\" and i < int(TRAIN_SIZE * num_lines):\n",
    "                    continue  # has not reached start of val part of data\n",
    "                if i>=int(num_lines):\n",
    "                    break # чтобы можно было грузить меньше картинок\n",
    "                elements = line.strip().split(\"\\t\")\n",
    "                if ignore_image is not None and elements[0] in ignore_image:\n",
    "                    print(\"ignore: \",elements[0])\n",
    "                    continue\n",
    "                    \n",
    "                image_name = os.path.join(images_root, elements[0])                \n",
    "                self.image_names.append(image_name)\n",
    "                \n",
    "                if split in (\"train\", \"val\"):\n",
    "                    landmarks = list(map(np.int16, elements[1:]))\n",
    "                    landmarks = np.array(landmarks, dtype=np.int16).reshape((len(landmarks) // 2, 2))                                                            \n",
    "                    self.landmarks.append(landmarks)\n",
    "#                     flip_landmarks = flip_lm_v(landmarks)\n",
    "#                     self.flip_landmarks.append(flip_landmarks)\n",
    "\n",
    "        if split in (\"train\", \"val\"):\n",
    "            self.landmarks = torch.as_tensor(self.landmarks)\n",
    "#             self.flip_landmarks = torch.as_tensor(self.flip_landmarks)\n",
    "        else:\n",
    "            self.landmarks = None\n",
    "#             self.flip_landmarks = None\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "\n",
    "        image = cv2.imread(self.image_names[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                \n",
    "        sample[\"image\"] = image\n",
    "        \n",
    "        if self.landmarks is not None:\n",
    "            landmarks = self.landmarks[idx].clone()            \n",
    "            sample[\"landmarks\"] = landmarks\n",
    "            \n",
    "#             flip_landmarks = self.flip_landmarks[idx].clone()            \n",
    "#             sample[\"flip_landmarks\"] = flip_landmarks\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "def restore_landmarks_batch_ex(landmarks, fs, margins_x, margins_y,dx,dy):\n",
    "    landmarks[:, :, 0] += margins_x[:, None]\n",
    "    landmarks[:, :, 1] += margins_y[:, None]\n",
    "    landmarks /= fs[:, None, None]\n",
    "    landmarks[:, :, 0] += dx[:, None]\n",
    "    landmarks[:, :, 1] += dy[:, None]\n",
    "    return landmarks\n",
    "\n",
    "def restore_landmarks_batch(landmarks, fs, margins_x, margins_y):\n",
    "    landmarks[:, :, 0] += margins_x[:, None]\n",
    "    landmarks[:, :, 1] += margins_y[:, None]\n",
    "    landmarks /= fs[:, None, None]    \n",
    "    return landmarks\n",
    "\n",
    "\n",
    "\n",
    "def create_submission(path_to_data, test_predictions, path_to_submission_file):\n",
    "    test_dir = os.path.join(path_to_data, \"test\")\n",
    "\n",
    "    output_file = path_to_submission_file\n",
    "    wf = open(output_file, 'w')\n",
    "    wf.write(SUBMISSION_HEADER)\n",
    "\n",
    "    mapping_path = os.path.join(test_dir, 'test_points.csv')\n",
    "    mapping = pd.read_csv(mapping_path, delimiter='\\t')\n",
    "\n",
    "    for i, row in mapping.iterrows():\n",
    "        file_name = row[0]\n",
    "        point_index_list = np.array(eval(row[1]))\n",
    "        points_for_image = test_predictions[i]\n",
    "        needed_points = points_for_image[point_index_list].astype(np.int)\n",
    "        wf.write(file_name + ',' + ','.join(map(str, needed_points.reshape(2 * len(point_index_list)))) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, landmarks):\n",
    "    for point in landmarks:\n",
    "        x, y = point.astype(np.int)\n",
    "        cv2.circle(image, (x, y), 1, (128, 0, 128), 1, -1)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3207\n"
     ]
    }
   ],
   "source": [
    "ignore_image = set()\n",
    "with open(fn_ignore, \"rt\") as fp:\n",
    "    for line in fp:\n",
    "        ignore_image.add(line.strip())\n",
    "print(len(ignore_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fn = fnn.mse_loss\n",
    "valid_loss_fn = MseW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline при обучении\n",
    "train_transforms = transforms.Compose([\n",
    "#     RandomFlipV(),\n",
    "    CropRandom(),    \n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),   \n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])\n",
    "# pipeline при валидации и предсказании\n",
    "val_transforms = transforms.Compose([\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6beb4d8e79247c6ae295bfef934d680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "Wall time: 493 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Reading data...\")\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), train_transforms, split=\"train\",size = data_size,ignore_image=ignore_image)\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=True,shuffle=True)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b096ad271c4ced94769a028b32902d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "val_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'train'), val_transforms, split=\"val\",size = data_size,ignore_image=ignore_image)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,drop_last=False,shuffle=False)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\")\n",
    "device = torch.device(\"cuda: 0\") if use_gpu else torch.device(\"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1c221fd1e047ea9763835224786e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cef70ff4054d408f191f12331cf581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 0:\ttrain loss: 4633.7798\tval loss: 28529.9043\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7afec614894f96bd027a9b7433f1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ef44e7be374d8c9059ea2d2e245bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 1:\ttrain loss: 4627.1426\tval loss: 27348.9082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069319b30184661b26fc08df31df86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210189ac37bf4b03867153c09baf39f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 2:\ttrain loss: 4500.0645\tval loss: 23642.5195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e9918b058c401a970c13a141711090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80feba9bf4943a1b228df5b30f8fdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 3:\ttrain loss: 4206.4839\tval loss: 21683.0996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e4dab10c424d2b80c560344ff3ecd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2423047ae6a4fc18d1ecd6491ff13e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 4:\ttrain loss: 4028.4275\tval loss: 16802.6562\n"
     ]
    }
   ],
   "source": [
    "# 2. train & validate\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(0,epochs1):    \n",
    "    train_loss = train(model, train_dataloader, train_loss_fn, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, valid_loss_fn, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.4f}\\tval loss: {:5.4f}\".format(epoch, train_loss, val_loss))    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"{prj_name}_best.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "    with open(f\"{prj_name}_\"+str(epoch)+\".pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92eec4b495d6481282d0e08f8e649f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded850416c014afab15eec3d8b5eb118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 5:\ttrain loss: 3906.6233\tval loss: 17815.5742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d337411555f486f86806d52a48536dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4914049af9f6461daff02e38c374678d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 6:\ttrain loss: 3728.7178\tval loss: 16106.8496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5817c4b5536f4e5e9cdd0fa8467ef139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb885a418c643cfae3a10ab0a01a582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 7:\ttrain loss: 3482.2825\tval loss: 13415.0400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f7224066b7451a9bb6b7ec67f80a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6d084f12ce45c6a520167f98f4318a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 8:\ttrain loss: 3292.9880\tval loss: 10293.0479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d03ad0c1f344919cd675efa0b90af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21baca519694494a376d9caa5d11979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch # 9:\ttrain loss: 3115.0859\tval loss: 8726.2803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17c753208154696821aa525aa884cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ba4e94e3714111a0621fb42f8d7dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='validation...', max=1.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #10:\ttrain loss: 2919.8362\tval loss: 7887.4600\n"
     ]
    }
   ],
   "source": [
    "# 2. train & validate\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(epochs1,epochs1+epochs2):    \n",
    "    train_loss = train(model, train_dataloader, train_loss_fn, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, valid_loss_fn, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.4f}\\tval loss: {:5.4f}\".format(epoch, train_loss, val_loss))    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"{prj_name}_best.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "    with open(f\"{prj_name}_\"+str(epoch)+\".pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f61721e0faa4cc1a2415c83dd9e46d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514607bff60a47a4824e5435a7cbfe64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='test prediction...', max=520.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b5cc71379940>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     model.load_state_dict(best_state_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{prj_name}_last_test_predictions.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     pickle.dump({\"image_names\": test_dataset.image_names,\n",
      "\u001b[1;32m<ipython-input-5-5303660e9814>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_PTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test prediction...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1106\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1109\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7900b404c2f3>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7900b404c2f3>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \"\"\"\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\toolkits\\miniconda3\\envs\\cv\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. predict last\n",
    "test_dataset = ThousandLandmarksDataset(os.path.join(data_dir, 'test'), val_transforms, split=\"test\")\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, pin_memory=True,\n",
    "                                  shuffle=False, drop_last=False)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n",
    "with open(f\"{prj_name}_last_test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                 \"landmarks\": test_predictions}, fp)\n",
    "\n",
    "create_submission(data_dir, test_predictions, f\"{prj_name}_last_submit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict best\n",
    "with open(f\"{prj_name}_best.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n",
    "with open(f\"{prj_name}_best_test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                 \"landmarks\": test_predictions}, fp)\n",
    "\n",
    "create_submission(data_dir, test_predictions, f\"{prj_name}_best_submit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
